#!/usr/bin/env python
# coding: utf-8
"""
Fineâ€‘tuning script for MedGemma (4â€‘bit QLoRA, multiâ€‘image per sample)
====================================================================

ë³€ê²½ ë° ê°œì„  ì‚¬í•­ (v0.2)
-----------------------
1. **Headless í™˜ê²½ ëŒ€ì‘**
   * `matplotlib` ë°±ì—”ë“œë¥¼ **Agg**ë¡œ ê³ ì •í•˜ì—¬ GUI ì°½ì´ ì—´ë ¤ ë©ˆì¶”ëŠ” í˜„ìƒ ì œê±°.
   * ì‹œê°í™” ë‹¨ê³„ì—ì„œ `plt.show()` ëŒ€ì‹  `plt.savefig()` í›„ ì¦‰ì‹œ `plt.close()` ì²˜ë¦¬.
2. **CLI í”Œë˜ê·¸ ì¶”ê°€**
   * `--no_vis`â€¯: ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸/ì´ë¯¸ì§€ ì‹œê°í™” ë‹¨ê³„ ìì²´ë¥¼ ê±´ë„ˆë›¸ ìˆ˜ ìˆìŒ.
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. Imports & CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, mimetypes, datetime, argparse

import torch
from PIL import Image

import matplotlib
matplotlib.use("Agg")  # â† GUI ì—†ëŠ” ì„œë²„ì—ì„œë„ ë©ˆì¶”ì§€ ì•Šë„ë¡
import matplotlib.pyplot as plt

from datasets import load_dataset
from accelerate import Accelerator
from transformers import (
    AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTConfig, SFTTrainer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
parser = argparse.ArgumentParser(description="Fineâ€‘tune MedGemma with QLoRA")
parser.add_argument("--model_path", default="/home/mts/ssd_16tb/member/jks/medgemma_reg2025/notebooks/medgemma-4b-it")
parser.add_argument("--train_json", default="/home/mts/ssd_16tb/member/jks/medgemma_reg2025/notebooks/data/preprocess_tile/make_json/train_json/train_json_clean.json")
parser.add_argument("--epochs", type=int, default=3)
parser.add_argument("--lr", type=float, default=5e-5)
parser.add_argument("--rank", type=int, default=8)
parser.add_argument("--no_vis", action="store_true", help="Disable sample visualization for headless run")
args = parser.parse_args()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Model & LoRA ì¤€ë¹„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
bnb_cfg = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_storage=torch.float32,
)

accel = Accelerator(mixed_precision="fp16")

with accel.main_process_first():
    model = AutoModelForImageTextToText.from_pretrained(
        args.model_path,
        quantization_config=bnb_cfg,
        torch_dtype=torch.float16,
        attn_implementation="eager",
    )
    model = prepare_model_for_kbit_training(model)

# â”€â”€â”€ Gradient checkpointing ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â‘  ë¨¼ì € ì „ì²´ ëª¨ë“ˆì˜ ì²´í¬í¬ì¸íŠ¸ë§ì„ **ì™„ì „íˆ ë„ê³ **
model.gradient_checkpointing_disable()

# â‘¡ **Visionâ€‘tower** ë ˆì´ì–´ì—ë§Œ ë‹¤ì‹œ ì¼­ë‹ˆë‹¤.
vt = model.vision_tower
vision_layers = vt.encoder.layers if hasattr(vt, "encoder") else vt.vision_model.encoder.layers
for blk in vision_layers:
    blk.gradient_checkpointing = True

# LoRA ì„¤ì •
targets = [n for n, m in model.named_modules() if isinstance(m, torch.nn.Linear)]
lora_cfg = LoraConfig(r=args.rank, lora_alpha=args.rank * 4, lora_dropout=0.05, target_modules=targets, task_type="CAUSAL_LM")
model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

processor = AutoProcessor.from_pretrained(args.model_path)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Dataset ë¡œë“œ & Sanityâ€‘check
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def is_valid(ex):
    return ex.get("image") and os.path.exists(ex["image"]) and ex.get("messages")

print("\n[ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ]")
with accel.main_process_first():
    ds = load_dataset("json", data_files={"train": args.train_json})["train"].filter(is_valid)
print(f"âœ… ë¡œë“œ ì™„ë£Œ. ì´ ìƒ˜í”Œ: {len(ds):,}")

# (1) ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸Â·ì´ë¯¸ì§€ ì‹œê° í™•ì¸
if not args.no_vis and accel.is_main_process:
    print("\n[ğŸ” ìƒ˜í”Œ ì‹œê°í™” & Prompt í™•ì¸]")
    for i in range(min(3, len(ds))):
        ex = ds[i]
        prompt = processor.apply_chat_template(ex["messages"], add_generation_prompt=False, tokenize=False).strip()
        print(f"\nâ”€ Sample {i} â”€\nPrompt:\n" + prompt)
        img_path = ex["image"]
        if os.path.exists(img_path):
            img = Image.open(img_path).convert("RGB")
            plt.imshow(img)
            plt.title(f"Sample {i}")
            plt.axis("off")
         #   plt.savefig(f"sanity_vis_{i}.png")  # íŒŒì¼ë¡œ ì €ì¥ í›„ ë‹«ê¸°
            plt.close()
        else:
            print("âš  ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ:", img_path)

# (2) ìœ ë‹ˆí¬ ë¼ë²¨ ë‹¤ì–‘ì„± ì²´í¬
print("\n[ğŸ§ª ë¼ë²¨ ë‹¤ì–‘ì„± ê²€ì‚¬]")
label_set = set()
for i in range(min(1000, len(ds))):
    for m in ds[i]["messages"]:
        if m["role"] == "assistant":
            for c in m["content"]:
                if c.get("text"):
                    label_set.add(c["text"].strip())
label_cnt = len(label_set)
print(f"ìœ ë‹ˆí¬ ë¼ë²¨ ìˆ˜: {label_cnt}")
if label_cnt < 2:
    print("âš  ê²½ê³ : ëª¨ë“  ìƒ˜í”Œì´ ë™ì¼í•œ ë¼ë²¨ì„ ê°–ê³  ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")

# (3) Processor í…ŒìŠ¤íŠ¸
print("\n[âš™ Processor ë§ˆìŠ¤í‚¹ í…ŒìŠ¤íŠ¸]")
_sample = [ds[i] for i in range(min(4, len(ds)))]
texts = [processor.apply_chat_template(s["messages"], add_generation_prompt=False, tokenize=False).strip() for s in _sample]
imgs = [[Image.open(s["image"]).convert("RGB")] for s in _sample]
encoded = processor(text=texts, images=imgs, padding=True, return_tensors="pt")
labels = encoded["input_ids"].clone()
labels[labels == processor.tokenizer.pad_token_id] = -100
label_tokens = (labels != -100).sum().item()
print(f"ìœ íš¨ ë¼ë²¨ í† í° ìˆ˜: {label_tokens}")
if label_tokens == 0:
    raise ValueError("âŒ ëª¨ë“  ë¼ë²¨ì´ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ë˜ì—ˆìŠµë‹ˆë‹¤. í•™ìŠµ ë¶ˆê°€!")
print("âœ… Processor í…ŒìŠ¤íŠ¸ í†µê³¼")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Collate í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_image(path: str):
    mime, _ = mimetypes.guess_type(path)
    return Image.open(path).convert("RGB") if mime and mime.startswith("image") else torch.load(path, map_location="cpu")


def collate(batch):
    batch = [ex for ex in batch if is_valid(ex)] or [ds[0]]  # ë¹ˆ ë°°ì¹˜ ë°©ì§€
    texts, images = [], []
    for ex in batch:
        texts.append(processor.apply_chat_template(ex["messages"], add_generation_prompt=False, tokenize=False).strip())
        images.append([load_image(ex["image"])])

    out = processor(text=texts, images=images, padding=True, return_tensors="pt")
    labels = out["input_ids"].clone()
    labels[labels == processor.tokenizer.pad_token_id] = -100

    start_id = processor.tokenizer.convert_tokens_to_ids("<start_of_image>")
    soft_id = processor.tokenizer.convert_tokens_to_ids("<image_soft_token>")
    for i, ids in enumerate(out["input_ids"]):
        for p in (ids == start_id).nonzero(as_tuple=True)[0]:
            q = p + 1
            while q < ids.size(0) and soft_id <= ids[q] < soft_id + 256:
                q += 1
            labels[i, p:q] = -100

    out["labels"] = labels
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Trainer ì„¤ì • & í•™ìŠµ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

stamp = datetime.datetime.now().strftime("%m%d_%H%M")
sft_cfg = SFTConfig(
    output_dir=f"qlora_ckpt_{stamp}",
    num_train_epochs=args.epochs,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=args.lr,
    lr_scheduler_type="cosine",
    warmup_ratio=0.05,
    logging_steps=50,
    save_strategy="epoch",
    fp16=True,
    max_seq_length=1024,
    dataloader_drop_last=True,
    label_names=["labels"],
    report_to=["tensorboard"],
)

trainer = SFTTrainer(model=model, args=sft_cfg, train_dataset=ds, data_collator=collate)

print("\n[ğŸš€ í•™ìŠµ ì‹œì‘]")
trainer.train()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Save
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
accel.wait_for_everyone()
if accel.is_main_process:
    out_dir = "/home/mts/ssd_16tb/member/jks/medgemma_reg2025/notebooks/fintuning_model/medgemma_2400_VIT_v0.1.1"
    trainer.save_model(out_dir)
    print("ëª¨ë¸ ì €ì¥ ì™„ë£Œ:", out_dir)
