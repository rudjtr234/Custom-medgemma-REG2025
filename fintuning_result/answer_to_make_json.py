
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os, glob, json, gc
from pathlib import Path
from tqdm import tqdm
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForImageTextToText
from peft import PeftModel
import os
import glob
import json
import gc
import re
import multiprocessing as mp
from collections import defaultdict, Counter
from pathlib import Path
from PIL import Image
from transformers import AutoProcessor, AutoModelForImageTextToText
import time
from peft import PeftModel
from transformers import AutoTokenizer


def clean_report(text):
    # ë§ˆí¬ì—… ì œê±°
    text = re.sub(r'<[^>]+>', '', text)

    # \\n â†’ ì‹¤ì œ ì¤„ë°”ê¿ˆ
    text = text.replace('\\n\\n', '\n\n').replace('\\n', '\n')

    # n/a ì œê±°
    text = re.sub(r'\b[nN]/?[aA]\b', '', text)

    # ë°˜ë³µ ë¬¸ì¥ ì œê±°
    lines = text.split('\n')
    seen = set()
    new_lines = []
    for line in lines:
        line = line.strip()
        line = re.sub(r'^#*\s*(Human|Assistant|Response|Report|Example)\s*[:ï¼š]?\s*', '', line, flags=re.IGNORECASE)
        if line and line not in seen:
            seen.add(line)
            new_lines.append(line)

    # No tumor present. ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¤ë©´ í•˜ë‚˜ë§Œ
    filtered = []
    for l in new_lines:
        if l == "No tumor present." and "No tumor present." in filtered:
            continue
        filtered.append(l)

    return '\n'.join(filtered).strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. ê²½ë¡œ ë° í™˜ê²½ ì„¤ì •

BASE_ID     = "/home/mts/ssd_16tb/member/jks/medgemma_reg2025/notebooks/medgemma-4b-it"
ADAPTER_ID  = "/home/mts/ssd_16tb/member/jks/medgemma_reg2025/notebooks/fintuning_model/medgemma_2400_VIT_v0.1.1"
MERGE_DIR   = "/home/mts/ssd_16tb/member/jks/medgemma_reg2025/models/medgemma-merged_2400_VIT_v0.1.1"
#IMG_DIR     = "/home/mts/ssd_16tb/member/jks/medgemma_reg2025/notebooks/data/tile_testphase1_preprocess_data"

IMG_DIR     = "/home/mts/ssd_16tb/member/jks/medgemma_reg2025/notebooks/data/tile_test_2400"

OUT_JSON    = "/home/mts/ssd_16tb/member/jks/medgemma_reg2025/notebooks/data/preprocess_tile/fintuning_result/REG2025_Medgemma_report_tile_VIT_v0.1.1.json"
DEVICE      = "cuda:0" if torch.cuda.is_available() else "cpu"

os.environ["NCCL_P2P_DISABLE"] = "1"
os.environ["NCCL_IB_DISABLE"]  = "1"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def merge_adapter_once():
    # ë³‘í•©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆëŠ” íŒŒì¼ íŒ¨í„´ë“¤
    model_patterns = ["pytorch_model.bin", "model.safetensors", "pytorch_model-*.bin"]

    def model_exists():
        for pattern in model_patterns:
            if "*" in pattern:
                if any(Path(MERGE_DIR).glob(pattern)):
                    return True
            else:
                if (Path(MERGE_DIR) / pattern).exists():
                    return True
        return False

    if Path(MERGE_DIR).exists():
        if model_exists():
            print(f"â–¶ ì´ë¯¸ ë³‘í•©ëœ ëª¨ë¸ì´ ì¡´ì¬í•©ë‹ˆë‹¤ â†’ {MERGE_DIR}")
            return
        else:
            print(f"âš ï¸ ë³‘í•© í´ë”ëŠ” ìˆìœ¼ë‚˜ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë³‘í•©ì„ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")

    print("â–¶ ì–´ëŒ‘í„° ë³‘í•© ì‹œì‘â€¦")

    try:
        # â‘  untied ìƒíƒœë¡œ ë¡œë”©
        model = AutoModelForImageTextToText.from_pretrained(
            BASE_ID,
            torch_dtype=torch.bfloat16,
            device_map={"": 0},
            tie_word_embeddings=False,
            local_files_only=True,
            trust_remote_code=True,
        )

        # â‘¡ PEFT ì–´ëŒ‘í„° ë³‘í•©
        model = PeftModel.from_pretrained(
            model,
            ADAPTER_ID,
            torch_dtype=torch.bfloat16,
            local_files_only=True
        )
        model = model.merge_and_unload()

        # â‘¢ lm_headë¥¼ embed_tokensì™€ ë¶„ë¦¬ëœ ê°ì²´ë¡œ ìƒì„±
        import torch.nn as nn
        new_lm_head = nn.Linear(
            model.language_model.embed_tokens.weight.shape[0],
            model.language_model.embed_tokens.weight.shape[1],
            bias=False
        ).to(dtype=torch.bfloat16)
        new_lm_head.weight.data = model.language_model.embed_tokens.weight.data.clone().detach()
        model.lm_head = new_lm_head

        # â‘£ ì €ì¥
        Path(MERGE_DIR).mkdir(parents=True, exist_ok=True)
        model.save_pretrained(MERGE_DIR, safe_serialization=True)

        processor = AutoProcessor.from_pretrained(BASE_ID, local_files_only=True)
        processor.save_pretrained(MERGE_DIR)

    except Exception as e:
        print(f"âŒ ë³‘í•© ë˜ëŠ” ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    # â‘¤ ì €ì¥ í™•ì¸ (sharded ëª¨ë¸ë„ í¬í•¨í•´ì„œ í™•ì¸)
    if model_exists():
        print(f"âœ… ë³‘í•© ì™„ë£Œ ë° ì €ì¥ ì„±ê³µ â†’ {MERGE_DIR}")
    else:
        print(f"âš ï¸ ë³‘í•©ì€ ë˜ì—ˆìœ¼ë‚˜ ëª¨ë¸ íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê²½ë¡œ í™•ì¸ í•„ìš” â†’ {MERGE_DIR}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = (
    "You are a pathologist AI assistant trained to analyze whole slide images (WSI).\n"
    "You are able to understand the visual content of histopathological slides and generate structured pathology reports.\n"
    "Your response must follow this exact format:\n"
    "[Organ/tissue], [procedure];\n"
    "[Findings]\n"
    "- If there are multiple findings, number them (1., 2., etc.), each on a new line.\n"
    "- Add notes only after a double newline (\\n\\n), and only if directly relevant to the specimen (e.g., \"Note) The specimen includes muscle proper.\")\n"
    "- For non-malignant cases, state: \"No tumor present.\"\n"
    "Strictly output only the report. Do not provide any explanation, commentary, or analysis beyond the formatted report itself. "
    "Do not refer to the image, model, reasoning, or any additional information.\n"
    "Examples (do not describe them, just use structure):\n"
    "Prostate, biopsy;\nAcinar adenocarcinoma, Gleason score 7 (4+3), grade group 3\n"
    "Breast, biopsy;\nMucinous carcinoma\n"
    "Urinary bladder, transurethral resection;\n1. Non-invasive papillary urothelial carcinoma, high grade\n2. Urothelial carcinoma in situ\n\nNote) The specimen includes muscle proper.\n"
    "Do not copy the above examples. Generate a report only based on the current image."
    "Output must follow the exact format above without any additional text or remarks."
)

def build_prompt() -> str:
    return (
        "### System:\n"
        f"{SYSTEM_PROMPT}\n"
        "### User:\n"
        "<start_of_image>\n"
        "### Assistant:\n"
    )

tokenizer = AutoTokenizer.from_pretrained(BASE_ID)

# ê·¸ ë‹¤ìŒì— GEN_KWARGS ì •ì˜
GEN_KWARGS = dict(
    max_new_tokens     = 128,
    num_beams          = 10,
    do_sample          = False,
    repetition_penalty = 1.2,
    pad_token_id       = tokenizer.eos_token_id,  # âœ… ì½¤ë§ˆ ì œê±°!
    eos_token_id       = tokenizer.eos_token_id
)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def generate_partial_reports(gpu_id, image_paths):
    print(f"[GPU {gpu_id}] {len(image_paths)}ê°œ ì´ë¯¸ì§€ ì¶”ë¡  ì‹œì‘")
    device = f"cuda:{gpu_id}"

    model = AutoModelForImageTextToText.from_pretrained(
        MERGE_DIR, torch_dtype=torch.bfloat16, device_map={"": device}
    )
    processor = AutoProcessor.from_pretrained(MERGE_DIR)
    GEN_KWARGS["eos_token_id"] = processor.tokenizer.convert_tokens_to_ids("###")
    GEN_KWARGS["return_dict_in_generate"] = True
    GEN_KWARGS["output_scores"] = False

    results = []
    total = len(image_paths)
    prev_time = time.time()

    for i, path in enumerate(image_paths, start=1):
        try:
            img = Image.open(path).convert("RGB")
        except Exception as e:
            print(f"[GPU {gpu_id}] ì´ë¯¸ì§€ ë¡œë”© ì‹¤íŒ¨: {path} - {e}", flush=True)
            continue

        try:
            prompt = str(build_prompt())
            inputs = processor(
                text=prompt,
                images=img,
                return_tensors="pt",
                padding=True
            ).to(device)

            print(f"[GPU {gpu_id}] ì…ë ¥ ìƒì„± ì™„ë£Œ - {os.path.basename(path)}")
        except Exception as e:
            print(f"[GPU {gpu_id}] processor ì…ë ¥ ìƒì„± ì‹¤íŒ¨: {e}", flush=True)
            continue

        try:
            with torch.no_grad():
                gen_out = model.generate(**inputs, **GEN_KWARGS)
                output_ids = gen_out.sequences[0]
            input_len = len(inputs["input_ids"][0])
            gen_output = output_ids[input_len:]  # ì…ë ¥ ë¶€ë¶„ ì œì™¸
            response = processor.decode(gen_output, skip_special_tokens=True)
            cleaned = clean_report(response)

            results.append({
                "id": os.path.splitext(os.path.basename(path))[0] + ".tiff",
                "report": cleaned
            })
        except Exception as e:
            print(f"[GPU {gpu_id}] ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨ - {path}: {e}", flush=True)
            continue

        # ì§„í–‰ë¥  ì¶œë ¥
        if i % 10 == 0 or i == total:
            now = time.time()
            elapsed = now - prev_time
            print(f"[GPU {gpu_id}] {i}/{total}ì¥ ì™„ë£Œ ({(i/total)*100:.1f}%) - 10ì¥ ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ", flush=True)
            prev_time = now

    # ê²°ê³¼ ì €ì¥
    tmp_file = f"{OUT_JSON}.gpu{gpu_id}.tmp"
    with open(tmp_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"[GPU {gpu_id}] ì™„ë£Œ â†’ {tmp_file}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_reports_multi_gpu(num_gpus=5):
    all_slide_dirs = sorted(glob.glob(os.path.join(IMG_DIR, "*")))[:100]  # ìŠ¬ë¼ì´ë“œ ë””ë ‰í† ë¦¬ ìƒí•œì„ 
    jpg_paths = []
    for slide_dir in all_slide_dirs:
        jpg_paths.extend(sorted(glob.glob(os.path.join(slide_dir, "*.jpg"))))

    print(f"\nğŸ“Š ì´ {len(jpg_paths):,}ì¥ì˜ ì´ë¯¸ì§€ë¥¼ {num_gpus}ê°œì˜ GPUë¡œ ë‚˜ëˆ„ì–´ ì¶”ë¡ í•©ë‹ˆë‹¤.\n")

    chunk_size = len(jpg_paths) // num_gpus
    chunks = [jpg_paths[i * chunk_size: (i + 1) * chunk_size] for i in range(num_gpus - 1)]
    chunks.append(jpg_paths[(num_gpus - 1) * chunk_size:])  # ë§ˆì§€ë§‰ì— ë‚˜ë¨¸ì§€ í¬í•¨

    processes = []
    start_time = time.time()

    for i in range(num_gpus):
        print(f"ğŸš€ GPU {i} â†’ {len(chunks[i]):,}ì¥ í• ë‹¹ë¨")
        p = mp.Process(target=generate_partial_reports, args=(i, chunks[i]))
        p.start()
        processes.append(p)

    for p in processes:
        p.join()

    # ê²°ê³¼ ë³‘í•©
    final_results = []
    for i in range(num_gpus):
        tmp_file = f"{OUT_JSON}.gpu{i}.tmp"
        print(f"ğŸ“¥ GPU {i} ê²°ê³¼ ë³‘í•© ì¤‘: {tmp_file}")
        with open(tmp_file, "r", encoding="utf-8") as f:
            partial = json.load(f)
            final_results.extend(partial)
        os.remove(tmp_file)

    with open(OUT_JSON, "w", encoding="utf-8") as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… íƒ€ì¼ ë‹¨ìœ„ ë¦¬í¬íŠ¸ {len(final_results):,}ê°œ ì €ì¥ ì™„ë£Œ â†’ {OUT_JSON}")

    # ëŒ€í‘œ ë¦¬í¬íŠ¸ ìƒì„±: ìŠ¬ë¼ì´ë“œ ID ì¶”ì¶œ ì‹œ .tiff í¬í•¨
    slide_report_map = defaultdict(list)
    for r in final_results:
        tile_id = os.path.basename(r["id"])
        slide_id = "_".join(tile_id.split("_")[:-2]) + ".tiff"
        slide_report_map[slide_id].append(r["report"])

    representative_results = []
    for slide_id, reports in slide_report_map.items():
        most_common_report = Counter(reports).most_common(1)[0][0]
        representative_results.append({
            "id": slide_id,
            "report": most_common_report
        })

    rep_out_json = OUT_JSON.replace(".json", "_representative.json")
    with open(rep_out_json, "w", encoding="utf-8") as f:
        json.dump(representative_results, f, ensure_ascii=False, indent=2)

    elapsed = time.time() - start_time
    print(f"ğŸ“ ìŠ¬ë¼ì´ë“œ ëŒ€í‘œ ë¦¬í¬íŠ¸ {len(representative_results):,}ê°œ ì €ì¥ ì™„ë£Œ â†’ {rep_out_json}")
    print(f"â± ì „ì²´ ì†Œìš” ì‹œê°„: {elapsed/60:.2f}ë¶„ ({elapsed:.1f}ì´ˆ)\n")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import multiprocessing as mp
    mp.set_start_method("spawn", force=True)
    merge_adapter_once()
    generate_reports_multi_gpu(num_gpus=4)
