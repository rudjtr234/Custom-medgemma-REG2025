from transformers import AutoProcessor, AutoModelForImageTextToText
from PIL import Image
from peft import PeftModel
import torch, os

# â”€â”€ 0. (RTX 40 ì‹œë¦¬ì¦ˆ ë“œë¼ì´ë²„ ê²½ê³  ë°©ì§€, í•„ìš” ì—†ìœ¼ë©´ ìƒëµ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ["NCCL_P2P_DISABLE"] = "1"
os.environ["NCCL_IB_DISABLE"] = "1"

# â”€â”€ ê²½ë¡œ ì§€ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
base_id    = "/home/mts/jks/medgemma/medgemma-4b-it"
adapter_id = "/home/mts/jks/medgemma/medgemma_reg2025/notebooks/fintuning_model/medgemma-REG_2025_06_16"
image_path = "/home/mts/jks/medgemma/medgemma_reg2025/notebooks/data/thumbnail_pngs_1000/PIT_01_02994_01.png"

device = "cuda" if torch.cuda.is_available() else "cpu"

# â”€â”€ ëª¨ë¸ + ì–´ëŒ‘í„° ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model = AutoModelForImageTextToText.from_pretrained(
    base_id, torch_dtype=torch.bfloat16, device_map="auto"
)
model = PeftModel.from_pretrained(model, adapter_id, torch_dtype=torch.bfloat16)
#model = model.to(device)

# â”€â”€ í”„ë¡œì„¸ì„œ ë¡œë“œ (ì–´ëŒ‘í„° ê²½ë¡œì— í† í¬ë‚˜ì´ì € ë³€ê²½ì‚¬í•­ì´ ìˆë‹¤ë©´ adapter_id ì‚¬ìš©) â”€â”€
processor = AutoProcessor.from_pretrained(base_id)

# â”€â”€ 2. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
image_path = "/home/mts/jks/medgemma/medgemma_reg2025/notebooks/data/thumbnail_pngs_1000/PIT_01_00004_01.png"

# â”€â”€ 3. ë©”ì‹œì§€(ChatML) êµ¬ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = (
    "You are a pathologist AI assistant trained to analyze whole slide images (WSI). "
    "You interpret the histopathological content and generate concise pathology reports.\n\n"
    "Please follow this structure:\n"
    "- Line 1: [Organ/tissue], [procedure];\n"
    "- Line 2: Findings (numbered if multiple)\n"
    "- Notes (optional): add after double newline (\\n\\n), starting with 'Note)'\n"
    "- For benign findings, use: 'No tumor present'\n\n"
)

messages = [
    {"role": "system", "content": [{"type": "text", "text": SYSTEM_PROMPT}]},
    {"role": "user",   "content": [
        {"type": "text",  "text": "Please provide a pathology report for this H&E-stained slide."},
        {"type": "image", "image": image_path}            # â† PIL ëŒ€ì‹  ê²½ë¡œ ë¬¸ìì—´
    ]},
]

prompt_str = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False).strip()

# (1) ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸, ë‘ ê²¹ -> í•œ ê²¹
inputs = processor(
    text   = [prompt_str],
    images = [Image.open(image_path).convert("RGB")],   # í•œ ê²¹!
    return_tensors = "pt",
    padding = True,
).to("cuda:0", dtype=torch.bfloat16)                   # ë‹¨ì¼ GPU ê¸°ì¤€

# (2) ì–´ëŒ‘í„°ê°€ ì‹¤ì œë¡œ í™œì„±í™”ë¼ ìˆëŠ”ì§€ ì²´í¬
print("active adapters:", model.active_adapters)       # ['default'] ê°€ ì•„ë‹ˆë©´
model.set_adapter("default")

# (3) ë””ë²„ê·¸ìš©: promptÂ·pixel_values í™•ì¸
print("prompt ì•ë¶€ë¶„:", prompt_str[:200])
print("pixel_values.shape:", inputs["pixel_values"].shape)  # (1, 3, 224, 224) ?

model.generation_config.do_sample = False
model.generation_config.pad_token_id = processor.tokenizer.eos_token_id
processor.tokenizer.padding_side = "left"

with torch.inference_mode():
    gen_ids = model.generate(
        **inputs,
        max_new_tokens=128,
        do_sample=True,        # ìƒ˜í”Œë§ ì¼œê¸°
        temperature=0.7,
        top_p=0.9,
    )

generated = gen_ids[0, inputs["input_ids"].shape[-1]:]
report = processor.decode(generated, skip_special_tokens=True)
print("\nğŸ” Generated Report:\n", report)
